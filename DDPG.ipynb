{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from gym.envs.registration import register\n",
    "from IPython.display import clear_output\n",
    "from itertools import count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2212)\n",
    "np.random.seed(2212)\n",
    "tf.random.set_seed(2212)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation_space: (3,)\n",
      "action_space: (1,)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pendulum-v0')\n",
    "print('observation_space:', env.observation_space.shape)\n",
    "print('action_space:', env.action_space.shape)\n",
    "\n",
    "act_limit = env.action_space.high[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, env):\n",
    "        self.gamma = 0.99\n",
    "        \n",
    "        self.env = env\n",
    "        state_dim = np.squeeze(env.observation_space.shape)\n",
    "        action_dim = np.squeeze(env.action_space.shape)\n",
    "        \n",
    "        self.state_dim, self.action_dim = state_dim, action_dim\n",
    "        \n",
    "        self.actor = Actor(state_dim, action_dim)\n",
    "        self.critic = Critic(state_dim, action_dim)\n",
    "        \n",
    "        self.target_actor = Actor(state_dim, action_dim)\n",
    "        self.target_critic = Critic(state_dim, action_dim)\n",
    "        \n",
    "        self.target_actor.model.set_weights(self.actor.model.get_weights())\n",
    "        self.target_critic.model.set_weights(self.critic.model.get_weights())\n",
    "        \n",
    "        self.memory = deque(maxlen=100000)\n",
    "        \n",
    "    def experience(self, e):\n",
    "        self.memory.append(e)\n",
    "        \n",
    "    def replay(self, size):\n",
    "        samples = random.sample(self.memory, size)\n",
    "        states, actions, next_states, rewards, dones = zip(*samples)\n",
    "        \n",
    "        states = np.asarray(states).reshape((-1, self.state_dim))\n",
    "        actions = np.asarray(actions).reshape((-1, self.action_dim))\n",
    "        next_states = np.asarray(next_states).reshape((-1, self.state_dim))\n",
    "        rewards = np.asarray(rewards).reshape((-1, 1))\n",
    "        dones = np.asarray(dones).reshape((-1, 1)).astype(np.float)\n",
    "        \n",
    "        states = tf.convert_to_tensor(states)\n",
    "        actions = tf.convert_to_tensor(actions)\n",
    "        next_states = tf.convert_to_tensor(next_states)\n",
    "        rewards = tf.convert_to_tensor(rewards)\n",
    "        dones = tf.convert_to_tensor(dones)\n",
    "        \n",
    "        return states, actions, next_states, rewards, dones\n",
    "        \n",
    "    @tf.function\n",
    "    def train(self, states, actions, next_states, rewards, dones):\n",
    "        with tf.GradientTape() as tape:\n",
    "            target_actions = self.target_actor.model(next_states)\n",
    "            next_values = self.target_critic.model([next_states, target_actions])\n",
    "            target_values = rewards + self.gamma * next_values * (1 - dones)\n",
    "            \n",
    "            values = self.critic.get_value(states, actions)\n",
    "            critic_loss = tf.reduce_mean(tf.square(target_values - values))\n",
    "            \n",
    "        critic_grad = tape.gradient(critic_loss, self.critic.model.trainable_variables)\n",
    "        self.critic.optimizer.apply_gradients(\n",
    "            zip(critic_grad, self.critic.model.trainable_variables)\n",
    "        )\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            actions = self.actor.model(states)\n",
    "            values = self.critic.model([states, actions])\n",
    "            actor_loss = -tf.reduce_mean(values)\n",
    "            \n",
    "        actor_grad = tape.gradient(actor_loss, self.actor.model.trainable_variables)\n",
    "        self.actor.optimizer.apply_gradients(\n",
    "            zip(actor_grad, self.actor.model.trainable_variables)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor():\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        \n",
    "        last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "        inputs = tf.keras.Input((state_dim,))\n",
    "        hidden = tf.keras.layers.Dense(64, activation='relu')(inputs)\n",
    "        #hidden = tf.keras.layers.BatchNormalization()(hidden)\n",
    "        hidden = tf.keras.layers.Dense(64, activation='relu')(hidden)\n",
    "        #hidden = tf.keras.layers.BatchNormalization()(hidden)\n",
    "        outputs = tf.keras.layers.Dense(\n",
    "            action_dim, activation='tanh', kernel_initializer=last_init\n",
    "            #action_dim, activation='linear'\n",
    "        )(hidden)\n",
    "\n",
    "        outputs = outputs * act_limit\n",
    "        model = tf.keras.Model(inputs, outputs)\n",
    "        \n",
    "        self.model = model\n",
    "        self.optimizer = tf.keras.optimizers.Adam(lr=0.001)\n",
    "        \n",
    "    def get_policy(self, states):\n",
    "        policy = self.model(tf.convert_to_tensor(states))\n",
    "        return policy\n",
    "        \n",
    "    def get_action(self, state, noise=True):\n",
    "        states = np.expand_dims(state, axis=0)\n",
    "        action = self.get_policy(states)\n",
    "        \n",
    "        if noise:\n",
    "            action += np.random.normal(0, 0.3)\n",
    "            \n",
    "        action = np.clip(action, -act_limit, act_limit)\n",
    "        return action\n",
    "    \n",
    "class Critic():\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        state_in = tf.keras.Input((state_dim,))\n",
    "        action_in = tf.keras.Input((action_dim,))\n",
    "        \n",
    "        concat = tf.keras.layers.concatenate([state_in, action_in])\n",
    "        hidden = tf.keras.layers.Dense(64, activation='relu')(concat)\n",
    "        #hidden = tf.keras.layers.BatchNormalization()(hidden)\n",
    "        hidden = tf.keras.layers.Dense(64, activation='relu')(hidden)\n",
    "        #hidden = tf.keras.layers.BatchNormalization()(hidden)\n",
    "        output = tf.keras.layers.Dense(1, activation='linear')(hidden)\n",
    "        \n",
    "        model = tf.keras.Model([state_in, action_in], output)\n",
    "        \n",
    "        self.model = model\n",
    "        self.optimizer = tf.keras.optimizers.Adam(lr=0.002)\n",
    "        \n",
    "    def get_value(self, states, actions):\n",
    "        states, actions = tf.convert_to_tensor(states), tf.convert_to_tensor(actions)\n",
    "        value = self.model([states, actions])\n",
    "        return value\n",
    "    \n",
    "agent = Agent(env)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def update_target(target_weights, weights, tau):\n",
    "    for (a, b) in zip(target_weights, weights):\n",
    "        a.assign(b * tau + a * (1 - tau))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tau = 0.005\n",
    "\n",
    "def run(agent, num_episodes, render=False):\n",
    "    for ep in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        \n",
    "        done = False\n",
    "        ep_reward = 0\n",
    "        for step in count():\n",
    "            action = agent.actor.get_action(state)[0]\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            if render:\n",
    "                env.render()\n",
    "            \n",
    "            ep_reward += reward\n",
    "            \n",
    "            agent.experience((state, action, next_state, reward, done))\n",
    "            \n",
    "            batch_size = 128\n",
    "            if len(agent.memory) > batch_size:\n",
    "                states, actions, next_states, rewards, dones = agent.replay(batch_size)\n",
    "                agent.train(states, actions, next_states, rewards, dones)\n",
    "                \n",
    "            update_target(agent.target_actor.model.variables, agent.actor.model.variables, tau)\n",
    "            update_target(agent.target_critic.model.variables, agent.critic.model.variables, tau)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            state = next_state\n",
    "                \n",
    "            #if step == 1: break\n",
    "        print(f'Episode: {ep}, ep_reward: {ep_reward}, step: {step+1}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, ep_reward: -1432.6894552208246, step: 200\n",
      "Episode: 1, ep_reward: -1537.8364227309914, step: 200\n",
      "Episode: 2, ep_reward: -1373.513556611526, step: 200\n",
      "Episode: 3, ep_reward: -1755.5402890398743, step: 200\n",
      "Episode: 4, ep_reward: -976.3706046331565, step: 200\n",
      "Episode: 5, ep_reward: -1276.3500523131042, step: 200\n",
      "Episode: 6, ep_reward: -1042.453425490905, step: 200\n",
      "Episode: 7, ep_reward: -986.4722966773998, step: 200\n",
      "Episode: 8, ep_reward: -968.824885582649, step: 200\n",
      "Episode: 9, ep_reward: -1322.797183737679, step: 200\n",
      "Episode: 10, ep_reward: -1135.6764687399452, step: 200\n",
      "Episode: 11, ep_reward: -1263.7223792446835, step: 200\n",
      "Episode: 12, ep_reward: -1011.7006643424007, step: 200\n",
      "Episode: 13, ep_reward: -515.3197731995912, step: 200\n",
      "Episode: 14, ep_reward: -1232.2215217177554, step: 200\n",
      "Episode: 15, ep_reward: -410.186722466114, step: 200\n",
      "Episode: 16, ep_reward: -131.77884883477253, step: 200\n",
      "Episode: 17, ep_reward: -128.37848905125225, step: 200\n",
      "Episode: 18, ep_reward: -130.4419829957967, step: 200\n",
      "Episode: 19, ep_reward: -134.74859828865468, step: 200\n",
      "Episode: 20, ep_reward: -8.545822129669203, step: 200\n",
      "Episode: 21, ep_reward: -135.92523130578476, step: 200\n",
      "Episode: 22, ep_reward: -349.5937293927769, step: 200\n",
      "Episode: 23, ep_reward: -370.647555967005, step: 200\n",
      "Episode: 24, ep_reward: -129.18178680836454, step: 200\n",
      "Episode: 25, ep_reward: -132.3319324121565, step: 200\n",
      "Episode: 26, ep_reward: -128.91989443103856, step: 200\n",
      "Episode: 27, ep_reward: -811.835071253038, step: 200\n",
      "Episode: 28, ep_reward: -245.26833486723206, step: 200\n",
      "Episode: 29, ep_reward: -127.04697945290269, step: 200\n",
      "Episode: 30, ep_reward: -120.92633857994332, step: 200\n",
      "Episode: 31, ep_reward: -121.5770677688229, step: 200\n",
      "Episode: 32, ep_reward: -257.3304902876328, step: 200\n",
      "Episode: 33, ep_reward: -3.7876771659145545, step: 200\n",
      "Episode: 34, ep_reward: -117.1531507178247, step: 200\n",
      "Episode: 35, ep_reward: -118.65261361952376, step: 200\n",
      "Episode: 36, ep_reward: -232.3070761903575, step: 200\n",
      "Episode: 37, ep_reward: -122.5739791155093, step: 200\n",
      "Episode: 38, ep_reward: -226.04839579339523, step: 200\n",
      "Episode: 39, ep_reward: -228.79483775006915, step: 200\n",
      "Episode: 40, ep_reward: -125.01797602765359, step: 200\n",
      "Episode: 41, ep_reward: -121.90736331176738, step: 200\n",
      "Episode: 42, ep_reward: -120.50011771459457, step: 200\n",
      "Episode: 43, ep_reward: -345.53001736712594, step: 200\n",
      "Episode: 44, ep_reward: -2.006042786506181, step: 200\n",
      "Episode: 45, ep_reward: -6.196702562196032, step: 200\n",
      "Episode: 46, ep_reward: -125.50138153563651, step: 200\n",
      "Episode: 47, ep_reward: -122.51780694534935, step: 200\n",
      "Episode: 48, ep_reward: -2.2836041604836192, step: 200\n",
      "Episode: 49, ep_reward: -235.7315539544671, step: 200\n",
      "Episode: 50, ep_reward: -4.6604661614205005, step: 200\n",
      "Episode: 51, ep_reward: -121.55809393651012, step: 200\n",
      "Episode: 52, ep_reward: -121.66024165540576, step: 200\n",
      "Episode: 53, ep_reward: -238.70820363978947, step: 200\n",
      "Episode: 54, ep_reward: -360.60099243619635, step: 200\n",
      "Episode: 55, ep_reward: -4.743142650521855, step: 200\n",
      "Episode: 56, ep_reward: -241.73763100427453, step: 200\n",
      "Episode: 57, ep_reward: -245.13568371156623, step: 200\n",
      "Episode: 58, ep_reward: -254.58140938237545, step: 200\n",
      "Episode: 59, ep_reward: -4.403781358333112, step: 200\n",
      "Episode: 60, ep_reward: -124.21012169399285, step: 200\n",
      "Episode: 61, ep_reward: -233.50367954282575, step: 200\n",
      "Episode: 62, ep_reward: -129.39382751188145, step: 200\n",
      "Episode: 63, ep_reward: -120.19932600362738, step: 200\n",
      "Episode: 64, ep_reward: -1.5496866342489837, step: 200\n",
      "Episode: 65, ep_reward: -2.0323469416745255, step: 200\n",
      "Episode: 66, ep_reward: -378.8108287734091, step: 200\n",
      "Episode: 67, ep_reward: -118.71189465487791, step: 200\n",
      "Episode: 68, ep_reward: -119.6119939696137, step: 200\n",
      "Episode: 69, ep_reward: -250.37171570258107, step: 200\n",
      "Episode: 70, ep_reward: -123.04495572751564, step: 200\n",
      "Episode: 71, ep_reward: -244.316878381627, step: 200\n",
      "Episode: 72, ep_reward: -121.12716167699939, step: 200\n",
      "Episode: 73, ep_reward: -116.33026641097976, step: 200\n",
      "Episode: 74, ep_reward: -124.01135455869368, step: 200\n",
      "Episode: 75, ep_reward: -124.69351829402179, step: 200\n",
      "Episode: 76, ep_reward: -245.2822871947155, step: 200\n",
      "Episode: 77, ep_reward: -129.670709702249, step: 200\n",
      "Episode: 78, ep_reward: -121.02217388739237, step: 200\n",
      "Episode: 79, ep_reward: -123.31328771502059, step: 200\n",
      "Episode: 80, ep_reward: -1.2967312900140944, step: 200\n",
      "Episode: 81, ep_reward: -3.295786914377285, step: 200\n",
      "Episode: 82, ep_reward: -128.43161180215978, step: 200\n",
      "Episode: 83, ep_reward: -282.9452759665919, step: 200\n",
      "Episode: 84, ep_reward: -228.2879993952017, step: 200\n",
      "Episode: 85, ep_reward: -233.678753434256, step: 200\n",
      "Episode: 86, ep_reward: -262.31107494987094, step: 200\n",
      "Episode: 87, ep_reward: -119.93587584249494, step: 200\n",
      "Episode: 88, ep_reward: -127.45313114437629, step: 200\n",
      "Episode: 89, ep_reward: -315.7634971012799, step: 200\n",
      "Episode: 90, ep_reward: -125.46777300261721, step: 200\n",
      "Episode: 91, ep_reward: -126.88771135523189, step: 200\n",
      "Episode: 92, ep_reward: -116.99301169653305, step: 200\n",
      "Episode: 93, ep_reward: -371.5473315224277, step: 200\n",
      "Episode: 94, ep_reward: -352.31984951537834, step: 200\n",
      "Episode: 95, ep_reward: -118.6264142017542, step: 200\n",
      "Episode: 96, ep_reward: -126.57562164765211, step: 200\n",
      "Episode: 97, ep_reward: -223.56812278907822, step: 200\n",
      "Episode: 98, ep_reward: -238.14398123010758, step: 200\n",
      "Episode: 99, ep_reward: -232.8859794829957, step: 200\n"
     ]
    }
   ],
   "source": [
    "run(agent, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, ep_reward: -239.48814787772773, step: 200\n",
      "Episode: 1, ep_reward: -127.1561848900932, step: 200\n",
      "Episode: 2, ep_reward: -232.95233327872737, step: 200\n",
      "Episode: 3, ep_reward: -126.02897310010258, step: 200\n",
      "Episode: 4, ep_reward: -0.9286318973483271, step: 200\n",
      "Episode: 5, ep_reward: -254.75131548443818, step: 200\n",
      "Episode: 6, ep_reward: -117.15766637515263, step: 200\n",
      "Episode: 7, ep_reward: -258.1157172919842, step: 200\n",
      "Episode: 8, ep_reward: -233.71333827836006, step: 200\n",
      "Episode: 9, ep_reward: -330.0635699215766, step: 200\n"
     ]
    }
   ],
   "source": [
    "run(agent, 10, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
